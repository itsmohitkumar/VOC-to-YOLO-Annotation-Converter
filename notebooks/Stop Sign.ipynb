{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "719d1cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # VOC to YOLO Converter and YOLOv5 Training\n",
    "# \n",
    "# This notebook walks through the process of:\n",
    "# 1. Downloading the dataset using Kaggle API.\n",
    "# 2. Converting Pascal VOC annotations into YOLO format.\n",
    "# 3. Setting up YOLOv5 for object detection.\n",
    "# 4. Training the YOLOv5 model on the custom dataset.\n",
    "# 5. Performing inference on test images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee5a4d7",
   "metadata": {},
   "source": [
    "#### Step 1: Get the Dataset from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f216215",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3794774139.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[14], line 8\u001b[0;36m\u001b[0m\n\u001b[0;31m    mkdir -p ~/.kaggle\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# %% [bash]\n",
    "# Install the Kaggle API\n",
    "%%bash\n",
    "%pip install kaggle\n",
    "\n",
    "# %% [bash]\n",
    "# Create a Kaggle directory and set the API key\n",
    "mkdir -p ~/.kaggle\n",
    "cp kaggle.json ~/.kaggle/kaggle.json\n",
    "chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "# %% [bash]\n",
    "# Download the dataset from Kaggle\n",
    "kaggle datasets download -d andrewmvd/road-sign-detection\n",
    "\n",
    "# %% [bash]\n",
    "# Unzip the downloaded dataset and organize it into the required folders\n",
    "unzip -q road-sign-detection.zip -d data\n",
    "mv ./data/annotations ./data/labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b7a997",
   "metadata": {},
   "source": [
    "#### Step 2: Set Up YOLOv5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3b51a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [bash]\n",
    "# Clone the YOLOv5 repository and install the necessary dependencies\n",
    "%%bash\n",
    "git clone https://github.com/ultralytics/yolov5\n",
    "cd yolov5 \n",
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e274de",
   "metadata": {},
   "source": [
    "#### Step 3: Convert Pascal VOC Annotations to YOLO Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85613c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def convert_box(image_size, box):\n",
    "    \"\"\"\n",
    "    Convert Pascal VOC bounding box to YOLO format.\n",
    "    \n",
    "    Args:\n",
    "    - image_size: tuple (width, height) of the image.\n",
    "    - box: list of bounding box coordinates [xmin, xmax, ymin, ymax].\n",
    "    \n",
    "    Returns:\n",
    "    - Normalized (x_center, y_center, width, height) in YOLO format.\n",
    "    \"\"\"\n",
    "    image_w, image_h = image_size\n",
    "    xmin, xmax, ymin, ymax = box\n",
    "    \n",
    "    # Center x, y coordinates\n",
    "    x_center = (xmin + xmax) / 2.0\n",
    "    y_center = (ymin + ymax) / 2.0\n",
    "    \n",
    "    # Width and height of the box\n",
    "    box_width = xmax - xmin\n",
    "    box_height = ymax - ymin\n",
    "    \n",
    "    # Normalize coordinates\n",
    "    x_center /= image_w\n",
    "    y_center /= image_h\n",
    "    box_width /= image_w\n",
    "    box_height /= image_h\n",
    "    \n",
    "    return x_center, y_center, box_width, box_height\n",
    "\n",
    "def convert_voc_to_yolo(label_dir='./data/labels', output_dir='./data/labels'):\n",
    "    \"\"\"\n",
    "    Convert Pascal VOC annotations (XML) to YOLO format and save them.\n",
    "    \n",
    "    Args:\n",
    "    - label_dir: Directory where the VOC XML files are stored.\n",
    "    - output_dir: Directory where the YOLO .txt files will be saved.\n",
    "    \"\"\"\n",
    "    class_names = ['trafficlight', 'speedlimit', 'crosswalk', 'stop']\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for annotation_file in os.listdir(label_dir):\n",
    "        file_base, file_ext = os.path.splitext(annotation_file)\n",
    "        \n",
    "        if file_ext != '.xml':\n",
    "            continue\n",
    "        \n",
    "        input_file_path = os.path.join(label_dir, annotation_file)\n",
    "        output_file_path = os.path.join(output_dir, f'{file_base}.txt')\n",
    "        \n",
    "        try:\n",
    "            tree = ET.parse(input_file_path)\n",
    "            root = tree.getroot()\n",
    "\n",
    "            # Extract image size\n",
    "            size_element = root.find('size')\n",
    "            image_w = int(size_element.find('width').text)\n",
    "            image_h = int(size_element.find('height').text)\n",
    "            \n",
    "            with open(output_file_path, 'w') as output_file:\n",
    "                for obj in root.iter('object'):\n",
    "                    class_name = obj.find('name').text\n",
    "                    if class_name not in class_names:\n",
    "                        continue  # Skip classes not in the predefined list\n",
    "                    \n",
    "                    # Skip difficult objects\n",
    "                    difficult = obj.find('difficult')\n",
    "                    if difficult is not None and int(difficult.text) == 1:\n",
    "                        continue\n",
    "                    \n",
    "                    # Get bounding box coordinates\n",
    "                    xml_box = obj.find('bndbox')\n",
    "                    bounding_box = [\n",
    "                        float(xml_box.find(tag).text) for tag in ('xmin', 'xmax', 'ymin', 'ymax')\n",
    "                    ]\n",
    "                    \n",
    "                    # Convert bounding box to YOLO format\n",
    "                    yolo_box = convert_box((image_w, image_h), bounding_box)\n",
    "                    class_id = class_names.index(class_name)\n",
    "                    \n",
    "                    # Write the YOLO format annotation to file\n",
    "                    output_file.write(f\"{class_id} \" + \" \".join(f\"{val:.6f}\" for val in yolo_box) + '\\n')\n",
    "        \n",
    "        except ET.ParseError:\n",
    "            print(f\"Error parsing XML file: {input_file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {input_file_path}: {e}\")\n",
    "\n",
    "# Run the VOC to YOLO conversion\n",
    "convert_voc_to_yolo()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371396ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [bash]\n",
    "# Create the custom YAML file for training\n",
    "%%bash\n",
    "echo \"\n",
    "path: ../data\n",
    "train: images  # directory containing training images\n",
    "val: images    # directory containing validation images\n",
    "\n",
    "# Define the class names\n",
    "names:\n",
    "  0: trafficlight\n",
    "  1: speedlimit\n",
    "  2: crosswalk\n",
    "  3: stop\n",
    "\" > yolov5/customVOC.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec9bec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [bash]\n",
    "# Start training the YOLOv5 model\n",
    "%%bash\n",
    "cd yolov5\n",
    "python train.py --img 320 --batch 16 --epochs 500 --data customVOC.yaml --weights yolov5s.pt --workers 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9975f644",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3df883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best trained model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'custom', path='yolov5/runs/train/exp/weights/best.pt', force_reload=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e52f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions on random images from the dataset\n",
    "fig, ax = plt.subplots(2, 4, figsize=(20, 10))\n",
    "imgs = os.listdir('./data/images')\n",
    "\n",
    "for idx in itertools.product(range(2), range(4)): \n",
    "    imgname = np.random.choice(imgs)\n",
    "    img = cv2.imread(f'./data/images/{imgname}')\n",
    "    \n",
    "    # Perform inference and render the result\n",
    "    results = model(img)\n",
    "    ax[idx[0], idx[1]].imshow(cv2.cvtColor(np.squeeze(results.render()), cv2.COLOR_BGR2RGB))\n",
    "    ax[idx[0], idx[1]].set_title(f\"Prediction: {imgname}\")\n",
    "    ax[idx[0], idx[1]].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
